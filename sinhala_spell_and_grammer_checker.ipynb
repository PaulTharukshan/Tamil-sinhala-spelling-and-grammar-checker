{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Standard Sinhala Words Dictionary\n",
        "standard_dictionary = {\n",
        "    \"මම\": \"මම\", \"ඔබ\": \"ඔබ\", \"අරලිය\": \"අරලිය\", \"ගඟ\": \"ගඟ\",\n",
        "    \"ගස්\": \"ගස්\", \"මල්\": \"මල්\", \"මලක්\": \"මලක්\", \"අපි\": \"අපි\",\n",
        "    \"අද\": \"අද\", \"ආදරය\": \"ආදරය\", \"සුන්දර\": \"සුන්දර\", \"විදුහල\": \"විදුහල\",\n",
        "    \"දින\": \"දින\", \"ගුරුවරුන්\": \"ගුරුවරුන්\", \"පෙරදිග\": \"පෙරදිග\", \"විදුහලට\": \"විදුහලට\",\n",
        "    \"මට\":\"මට\", \"යනවා\":\"යනවා\", \"කරති\":\"කරති\"\n",
        "}\n",
        "\n",
        "# Regional Variations and Common Typing Mistakes\n",
        "regional_variations = {\n",
        "    \"ගුරැවරුන්\": \"ගුරුවරුන්\", \"ගඟට\": \"ගඟට\", \"අපේ\": \"අපේ\",\n",
        "    \"මලක්ට\": \"මලක්\", \"අපිටා\": \"අපිට\", \"විදුහලටා\": \"විදුහලට\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "omgkAJiROT2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "def tokenize_sinhala_text(paragraph):\n",
        "    \"\"\"Tokenize Sinhala text while preserving sentence structure.\"\"\"\n",
        "    return indic_tokenize.trivial_tokenize(paragraph, lang=\"si\")\n"
      ],
      "metadata": {
        "id": "bHsJ9COcOn3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import Levenshtein\n",
        "\n",
        "def correct_spelling(word):\n",
        "    \"\"\"Correct spelling using standard dictionary, regional variations, and contextual suggestions.\"\"\"\n",
        "    if word in standard_dictionary:\n",
        "        return word  # Already correct\n",
        "    elif word in regional_variations:\n",
        "        return regional_variations[word]  # Regional variation\n",
        "\n",
        "    # Log unknown words\n",
        "    print(f\"Unknown Word: {word}\")\n",
        "\n",
        "    # Find the closest match in the dictionary\n",
        "    threshold = 1 if len(word) <= 4 else 2\n",
        "    closest_match = min(standard_dictionary.keys(), key=lambda x: Levenshtein.distance(word, x))\n",
        "    if Levenshtein.distance(word, closest_match) <= threshold:\n",
        "        return closest_match\n",
        "\n",
        "    # Return original word if no match found\n",
        "    return word\n"
      ],
      "metadata": {
        "id": "8CljMEeYOqkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def advanced_grammar_checker(paragraph):\n",
        "    \"\"\"Grammar checker with basic and advanced rules.\"\"\"\n",
        "    errors = []\n",
        "    words = paragraph.split()\n",
        "\n",
        "    # Rule 1: Detect repeated words\n",
        "    for i in range(len(words) - 1):\n",
        "        if words[i] == words[i + 1]:\n",
        "            errors.append(f\"Repeated word: '{words[i]}' at position {i}\")\n",
        "\n",
        "    # Rule 2: Sentence-ending errors\n",
        "    if paragraph.endswith(\"අ\"):\n",
        "        errors.append(\"Suggestion: Sentence should not end with 'අ'.\")\n",
        "\n",
        "    # Rule 3: Subject-Verb-Object (SVO) alignment\n",
        "    if len(words) > 2 and words[0] in [\"මම\", \"ඔබ\", \"අපි\"] and words[1] not in [\"යන්න\", \"යමු\"]:\n",
        "        errors.append(f\"Check sentence structure: Subject '{words[0]}' might need verb alignment.\")\n",
        "\n",
        "    return errors\n"
      ],
      "metadata": {
        "id": "xrAyaH3gOuwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "\n",
        "# Load IndicBERT model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"ai4bharat/indic-bert\")\n",
        "\n",
        "def contextual_correction(sentence):\n",
        "    \"\"\"Use IndicBERT to suggest corrections based on context.\"\"\"\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    corrected_tokens = []\n",
        "\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token not in standard_dictionary:\n",
        "            token = tokenizer.mask_token\n",
        "        corrected_tokens.append(token)\n",
        "\n",
        "    masked_sentence = tokenizer.convert_tokens_to_string(corrected_tokens)\n",
        "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs).logits\n",
        "\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token == tokenizer.mask_token:\n",
        "            masked_index = inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)\n",
        "            predicted_token_id = torch.argmax(outputs[0, masked_index]).item()\n",
        "            tokens[i] = tokenizer.decode([predicted_token_id])\n",
        "\n",
        "    return tokenizer.convert_tokens_to_string(tokens)\n"
      ],
      "metadata": {
        "id": "MnUxng99Oz9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correction_pipeline(paragraph):\n",
        "    \"\"\"Pipeline for spelling and grammar correction.\"\"\"\n",
        "    #  Tokenize\n",
        "    tokens = tokenize_sinhala_text(paragraph)\n",
        "\n",
        "    #  Correct Spelling\n",
        "    corrected_tokens = [correct_spelling(word) for word in tokens]\n",
        "\n",
        "    # Combine corrected tokens\n",
        "    corrected_paragraph = ' '.join(corrected_tokens)\n",
        "\n",
        "    #  Grammar Check\n",
        "    grammar_suggestions = advanced_grammar_checker(corrected_paragraph)\n",
        "\n",
        "    #  Contextual Correction\n",
        "    final_paragraph = contextual_correction(corrected_paragraph)\n",
        "\n",
        "    return final_paragraph, grammar_suggestions\n"
      ],
      "metadata": {
        "id": "-meAH9rOO1zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_paragraph = \"අපි විදුහලට යනවා. ගුරැවරුන් මට ආදරය කරති.\"\n",
        "\n",
        "# Execute the pipeline\n",
        "corrected_paragraph, grammar_suggestions = correction_pipeline(input_paragraph)\n",
        "\n",
        "# Display results\n",
        "print(\"Original Paragraph:\")\n",
        "print(input_paragraph)\n",
        "print(\"\\nCorrected Paragraph:\")\n",
        "print(corrected_paragraph)\n",
        "print(\"\\nGrammar Suggestions:\")\n",
        "for suggestion in grammar_suggestions:\n",
        "    print(f\"- {suggestion}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBO0h-8ZO7FU",
        "outputId": "e304591e-d8aa-43f7-d706-35599603f742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unknown Word: .\n",
            "Unknown Word: .\n",
            "Original Paragraph:\n",
            "අපි විදුහලට යනවා. ගුරැවරුන් මට ආදරය කරති.\n",
            "\n",
            "Corrected Paragraph:\n",
            "අප වදහලට යනව . ගරවරන මට ආදරය කරත .\n",
            "\n",
            "Grammar Suggestions:\n",
            "- Check sentence structure: Subject 'අපි' might need verb alignment.\n"
          ]
        }
      ]
    }
  ]
}